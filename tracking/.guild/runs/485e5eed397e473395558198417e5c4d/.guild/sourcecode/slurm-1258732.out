[stance1:train] Activating env....
Activating env [py38ptpl]
guild_version:             0.8.0
guild_install_location:    /home/serrano/miniconda3/envs/py38ptpl/lib/python3.8/site-packages/guild
guild_home:                /home/serrano/projects/SMM4H22/tracking/.guild
guild_resource_cache:      /home/serrano/projects/SMM4H22/tracking/.guild/cache/resources
installed_plugins:         config_flags, cpu, dask, disk, dvc, exec_script, gpu, ipynb, keras, memory, perf, python_script, queue, skopt
python_version:            3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
python_exe:                /home/serrano/miniconda3/envs/py38ptpl/bin/python
platform:                  Linux 4.18.0-193.el8.x86_64 x86_64
psutil_version:            5.8.0
tensorboard_version:       2.6.0
cuda_version:              11.4
nvidia_smi_version:        470.82.01
latest_guild_version:      0.8.1
A newer version of Guild AI is available. Run 'pip install guildai --upgrade' to install it.
*** Check tha [guild_home] is correctly set ***
===============================================
*** Available operations (execute _guild help_ for more info) ***
Refreshing flags...
WARNING: cannot import flags from src/stance_preprocess.py: ModuleNotFoundError: No module named 'fasttext' (run with guild --debug for details)
WARNING: cannot import flags from src/prepare_txts.py: ModuleNotFoundError: No module named 'negspacy' (run with guild --debug for details)
prepare_data                      Generate Dataframes with hydrated examples from previously processed splits, texts, annotations
prepare_splits                    Create a summary table with reference to all files and assign a split to each entry
prepare_submission                from predictions in DF (subtask 2 & 3 ) generate ann files for submission
prepare_task2_stance              Prepare official training data for stance Task2
prepare_texts                     Preprocess txt files (clean, segment, etc.)
base-model:evaluate               Run official evalation script
stance1:inference                 apply transforemr
stance1:inference-sentiment       apply transforemr
stance1:pipeline_inference_event  
stance1:train                     Train model Transformer based classifier
Running script ....
New Run
Refreshing flags...
WARNING: cannot import flags from src/stance_preprocess.py: ModuleNotFoundError: No module named 'fasttext' (run with guild --debug for details)
WARNING: cannot import flags from src/prepare_txts.py: ModuleNotFoundError: No module named 'negspacy' (run with guild --debug for details)
Resolving prepare_task2_stance dependency
Using run e63d44cad9bd43bd987b3e3e6ff7c889 for prepare_task2_stance resource
INFO: [numexpr.utils] Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
==> Setting [max_epochs] to 1 due to t10sec var
INFO: [pytorch_lightning.utilities.seed] Global seed set to 42
*** t10sec
Reading dataset.
**Training Data size: 22
Prepared for [0] extra features

========= Classifier:: NumLabels [labels|3,Premise|2] ================
Some weights of the model checkpoint at /home/serrano/projects/SMM4H22/tracking/.guild/runs/af7f2688c3424e0d985187675b004cd1/checkpoints/best were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Configuring TensorLogger output to --> /home/serrano/projects/SMM4H22/tracking/tensorboard_logs
INFO: [pytorch_lightning.utilities.rank_zero] Multiprocessing is handled by SLURM.
INFO: [pytorch_lightning.utilities.rank_zero] GPU available: True, used: True
INFO: [pytorch_lightning.utilities.rank_zero] TPU available: False, using: 0 TPU cores
INFO: [pytorch_lightning.utilities.rank_zero] IPU available: False, using: 0 IPUs
INFO: [pytorch_lightning.utilities.rank_zero] HPU available: False, using: 0 HPUs
INFO: [root] ... Setup [fit]
Reading dataset.
INFO: [root] 
---- Mapping split [train] -----
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 75.17ba/s]
INFO: [root] 
---- Mapping split [validation] -----
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 79.39ba/s]
INFO: [plibs.estimators.sequence_classifier_mtl] --> SetUp Model ** fit :: device cpu [<class 'torch.device'>]
/home/serrano/miniconda3/envs/py38ptpl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2203: LightningDeprecationWarning: `Trainer.gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` or `Trainer.device_ids` to get device information instead.
  rank_zero_deprecation(
INFO: [pytorch_lightning.accelerators.gpu] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: [plibs.estimators.sequence_classifier_mtl] *** Prepare optimizer and schedule (linear warmup and decay)  ***
INFO: [plibs.estimators.sequence_classifier_mtl] Optimizer LR: 1e-05 EPS: 0.001
INFO: [plibs.estimators.sequence_classifier_mtl] Only Optimizer --> AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 0.001
    lr: 1e-05
    weight_decay: 0.0

Parameter Group 1
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 0.001
    lr: 1e-05
    weight_decay: 0.0
)
INFO: [pytorch_lightning.callbacks.model_summary] 
  | Name         | Type                                          | Params
-------------------------------------------------------------------------------
0 | custom_model | ModelForSequenceClassificationMtlPlusFeatures | 338 M 
-------------------------------------------------------------------------------
338 M     Trainable params
0         Non-trainable params
338 M     Total params
1,353.183 Total estimated model params size (MB)
INFO: [pytorch_lightning.trainer.connectors.signal_connector] SLURM auto-requeueing enabled. Setting signal handlers.
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([16, 1024]) | cuda:0)
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.37it/s]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([6, 1024]) | cuda:0)
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.29it/s]                                                                           Training: 0it [00:00, ?it/s]Epoch 0:   0%|          | 0/8 [00:00<?, ?it/s]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([4, 1024]) | cuda:0)
Epoch 0:  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s]Epoch 0:  12%|█▎        | 1/8 [00:00<00:04,  1.70it/s, loss=2.12, v_num=821d]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([4, 1024]) | cuda:0)
Epoch 0:  25%|██▌       | 2/8 [00:00<00:02,  2.08it/s, loss=2.12, v_num=821d]Epoch 0:  25%|██▌       | 2/8 [00:00<00:02,  2.08it/s, loss=2, v_num=821d]   DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([4, 1024]) | cuda:0)
Epoch 0:  38%|███▊      | 3/8 [00:01<00:02,  2.25it/s, loss=2, v_num=821d]Epoch 0:  38%|███▊      | 3/8 [00:01<00:02,  2.25it/s, loss=1.92, v_num=821d]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([4, 1024]) | cuda:0)
Epoch 0:  50%|█████     | 4/8 [00:01<00:01,  2.35it/s, loss=1.92, v_num=821d]Epoch 0:  50%|█████     | 4/8 [00:01<00:01,  2.35it/s, loss=1.88, v_num=821d]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([4, 1024]) | cuda:0)
Epoch 0:  62%|██████▎   | 5/8 [00:02<00:01,  2.40it/s, loss=1.88, v_num=821d]Epoch 0:  62%|██████▎   | 5/8 [00:02<00:01,  2.40it/s, loss=1.81, v_num=821d]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([2, 1024]) | cuda:0)
Epoch 0:  75%|███████▌  | 6/8 [00:02<00:00,  2.55it/s, loss=1.81, v_num=821d]Epoch 0:  75%|███████▌  | 6/8 [00:02<00:00,  2.55it/s, loss=1.8, v_num=821d] 
Validation: 0it [00:00, ?it/s][A
Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][ADEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([16, 1024]) | cuda:0)

Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.89it/s][AEpoch 0:  88%|████████▊ | 7/8 [00:02<00:00,  2.39it/s, loss=1.8, v_num=821d]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([6, 1024]) | cuda:0)

Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.97it/s][AEpoch 0: 100%|██████████| 8/8 [00:03<00:00,  2.61it/s, loss=1.8, v_num=821d]Epoch 0: 100%|██████████| 8/8 [00:03<00:00,  2.60it/s, loss=1.8, v_num=821d]
                                                                      [AEpoch 0: 100%|██████████| 8/8 [00:03<00:00,  2.60it/s, loss=1.8, v_num=821d]INFO: [pytorch_lightning.utilities.rank_zero] Epoch 0, global step 6: 'validation_loss' reached 1.91104 (best 1.91104), saving model to '/home/serrano/projects/SMM4H22/tracking/.guild/runs/6987821dacfc44fcb6ef429eb42c1723/checkpoints/model_00_06_1.91.ckpt' as top 1
Epoch 0: 100%|██████████| 8/8 [00:47<00:00,  5.98s/it, loss=1.8, v_num=821d]
Converting to raw PT [/home/serrano/projects/SMM4H22/tracking/.guild/runs/6987821dacfc44fcb6ef429eb42c1723/checkpoints/model_00_06_1.91.ckpt]

========= Classifier:: NumLabels [labels|3,Premise|2] ================
Some weights of the model checkpoint at /home/serrano/projects/SMM4H22/tracking/.guild/runs/af7f2688c3424e0d985187675b004cd1/checkpoints/best were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

=========  TESTING  ==========
INFO: [root] ... Setup [test]
Reading dataset.
INFO: [root] 
---- Mapping split [test] -----
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 73.93ba/s]
INFO: [plibs.estimators.sequence_classifier_mtl] --> SetUp Model ** test :: device cpu [<class 'torch.device'>]
INFO: [pytorch_lightning.utilities.rank_zero] Restoring states from the checkpoint path at /home/serrano/projects/SMM4H22/tracking/.guild/runs/6987821dacfc44fcb6ef429eb42c1723/checkpoints/model_00_06_1.91.ckpt
INFO: [pytorch_lightning.accelerators.gpu] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
INFO: [pytorch_lightning.utilities.rank_zero] Loaded model weights from checkpoint at /home/serrano/projects/SMM4H22/tracking/.guild/runs/6987821dacfc44fcb6ef429eb42c1723/checkpoints/model_00_06_1.91.ckpt
Testing: 0it [00:00, ?it/s]Testing DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([16, 1024]) | cuda:0)
Testing DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]DEBUG: [plibs.estimators.model_seqcls_mtl] Pooled output:: torch.Size([6, 1024]) | cuda:0)
Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│   Premise_test_Accuracy   │    0.4545454680919647     │
│      Premise_test_F1      │    0.3122529685497284     │
│   labels_test_Accuracy    │    0.3181818127632141     │
│      labels_test_F1       │    0.22029824554920197    │
│         test_loss         │    1.8268307447433472     │
└───────────────────────────┴───────────────────────────┘
test_loss: 1.8268307447433472
labels_test_Accuracy: 0.3181818127632141
labels_test_F1: 0.22029824554920197
Premise_test_Accuracy: 0.4545454680919647
Premise_test_F1: 0.3122529685497284
... Saving predictions to [test_predictions.csv]
---------- FINALIZED -------------
