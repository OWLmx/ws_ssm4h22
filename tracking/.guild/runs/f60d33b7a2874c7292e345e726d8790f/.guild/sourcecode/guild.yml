
# - include: guild_base_models.yml

- config: shared-resources
  sourcecode:
    - include: 'src/'
    - exclude: '.dvcignore'
    - exclude: 'data/'
    - exclude: 'notebooks/'
    - exclude: 'models/'
  resources:
    data-input-train:
      - file: data/input/trainingdata_v3/train
        select: .+\.ann
        target-type: link
        target-path: input/train
    data-input-test:
      - file: data/input/trainingdata_v3/dev
        select: .+\.ann
        target-type: link
        target-path: input/test
    data-input-files:
      - file: data/input/trainingdata_v3
        select: .+\.(txt|ann)
        target-type: link
        target-path: input/files        
        
- config: log-flags
  name: Configuration of logs
  flags:
    logs_path: 
      description: Path where tensorboard logs will be written (for PL). Relative to the project's home
      default: tracking/tensorboard_logs

# -------------------------- General ops an base-model (abstract)   ----------------------------------------
-           
  extends: shared-resources
  operations:
    prepare_task2_stance:
      description: Prepare official training data for stance Task2
      main: src/stance_preprocess
      flags-import: all
      flags:
        t10sec: False
        dataset_type: covidlies # kglandt
        # in_path: /home/owlmx/research/comps/SMM4H22/data/input/task2
        in_path: /home/owlmx/research/comps/SMM4H22/data/input/task2/covidlies
        out_path: prepared
      # requires:
      #   - data-input-train
      #   - data-input-test
    # offcial CMED.pdf => The remaining annotations in 400 notes are split into 75% for training, 5% for development and 20% for test.

    prepare_submission_task2:
      description: from predictions in DF (subtask 2 & 3 ) generate ann files for submission
      main: src/prepare_submission_task2
      flags-import: all
      flags: 
        t10sec: False
        prediction_data: predictions.tsv
        subtask: 2a
        yhat_field: stance_yhat # yhat
        original_data: /home/owlmx/research/comps/SMM4H22/data/input/task2/official_test/test.tsv
      requires:
        - operation: inference
          select: predictions\.tsv
          # target-path: prepared
          target-type: link



- model: base-model
  extends: shared-resources
  operations:
    evaluate:
      description: Run official evalation script
      main: src/eval_script
      flags-import: all
      flags:
        folder1: gs_set
        folder2: gs_set
      output-scalars:        
        - 'Evt_Overall \(micro\) +(\value) +(\value) +(\value) +(?P<Evt_m_prec>\value) +(?P<Evt_m_recall>\value) +(?P<Evt_m_f1>\value)'
        - 'Evt_Overall \(macro\) +(\value) +(\value) +(\value) +(?P<Evt_M_prec>\value) +(?P<Evt_M_recall>\value) +(?P<Evt_M_f1>\value)'
        - 'Ctx_Overall \(micro\) +(\value) +(\value) +(\value) +(?P<Ctx_m_prec>\value) +(?P<Ctx_m_recall>\value) +(?P<Ctx_m_f1>\value)'
        - 'Ctx_Overall \(macro\) +(\value) +(\value) +(\value) +(?P<Ctx_M_prec>\value) +(?P<Ctx_M_recall>\value) +(?P<Ctx_M_f1>\value)'
        - 'Combined +(\value) +(\value) +(\value) +(?P<Comb_prec>\value) +(?P<Comb_recall>\value) +(?P<Comb_f1>\value)'


# --------------------------------  EVENT models (experiments)  ----------------------------------



- model: stance1
  extends: shared-resources
  description: stance classification 1
  operations:
    train:
      description: Train model Transformer based classifier 
      main: src/seqcls_transformer_train
      flags:
        $include:
          - log-flags
        t10sec: True
        gpus: 0
        model_name_or_path: 
          description: Name or path of the pretrained model to be used
          # default: distlilbert-base-uncased
          # default: digitalepidemiologylab/covid-twitter-bert-v2
          default: /home/owlmx/research/comps/SMM4H22/tracking/.guild/runs/af7f2688c3424e0d985187675b004cd1/checkpoints/best
        tokenizer_type: digitalepidemiologylab/covid-twitter-bert-v2
        task_name: sentclaim # base
        max_seq_length: 160
        # train_batch_size: 8
        train_batch_size: 32
        eval_batch_size: 32
        encoder_learning_rate: 1e-05
        # learning_rate: 2e-5
        learning_rate: 1e-05
        # warmup_steps: 700
        warmup_steps: 
          default: 0
          # default: 3000
          description: if < 0 then is interpreted as percentage
        max_epochs: 10
        adam_epsilon: 1e-3
        patience: 3
        auto_lr_find:
          description: Enables the pytorch-lightning's task of automatic finding a good LR 
          default: False
        data_dirpath: 
          description: Path of the directory where the datafiles are located
          default: prepared
        data_filename_prefix:
          description: Datafile's name without the suffix related to the split identification
          default: task2_ 
        data_filename_type: 
          description: Type of data files (csv | tsv | pkl )
          default: tsv
        # data_split_train:
        #   description: Suffix that identfies the split, if None the split won't be used
        #   default: train
        # data_split_valid:
        #   description: Suffix that identfies the split, if None the split won't be used
        #   default: valid
        # data_split_test:
        #   description: Suffix that identfies the split, if None the split won't be used
        #   default: test
        # fold_data_file: texts_processed.csv
        # fold_test: 6
        # fold_valid: 4
        use_weights_of: Null
        # use_weights_of: /home/serrano/projects/misinfocovid_wp2/tracking/.guild/runs/86deb642eb644c348d759ed3d2f0f3b1/checkpoints/best_model.ckpt
        stratified_batch_sampling: True
        # fields_transformations:  text|mask_term
        label_encoder: Null
      requires:
        - operation: prepare_task2_stance
          select: .*prepared\/.+\.(tsv|pkl)
          target-path: prepared
          target-type: link

    inference:
      description: apply transforemr 
      main: src/infer-cls_with_transformer
      flags:
        t10sec: False
        # tgtdata_path: /home/owlmx/idsia/misinfofastcatch_wp1/data/input/covid_infodemic/covid19_infodemic_english_data_extended.tsv
        tgtdata_path: prepared/task2_test.tsv
        model_path: model
        model_type: digitalepidemiologylab/covid-twitter-bert-v2
        num_labels: 3
        input_features: Claim2,tweet_text_clean
        label_mapper: 0|AGAINST,1|FAVOR,2|NONE # model/label_encoder.pkl 
        output_logits: yes
        rs_prefix: ""
      requires:
        - operation: train
          select: .*checkpoints\/best\/.+\.*
          target-path: model
          target-type: link
        - operation: prepare_task2_stance
          select: .*prepared\/.+\.(tsv|pkl)
          target-path: prepared
          target-type: link

    inference-sentiment:
      description: apply transforemr 
      main: src/infer-cls_with_transformer
      flags:
        t10sec: False
        # tgtdata_path: /home/owlmx/idsia/misinfofastcatch_wp1/data/input/covid_infodemic/covid19_infodemic_english_data_extended.tsv
        tgtdata_path: prepared/task2_train.tsv,prepared/task2_valid.tsv,prepared/task2_test.tsv
        model_path: cardiffnlp/twitter-roberta-base-sentiment-latest
        model_type: cardiffnlp/twitter-roberta-base-sentiment-latest
        num_labels: 3
        input_features: tweet_text_clean
        label_mapper: 0|Negative,1|Neutral,2|Positive 
        output_logits: yes
        rs_prefix: ""
      requires:
        - operation: train
          select: .*checkpoints\/best\/.+\.*
          target-path: model
          target-type: link
        - operation: prepare_task2_stance
          select: .*prepared\/.+\.(tsv|pkl)
          target-path: prepared
          target-type: link




    pipeline_inference_event:
      flags:
        t10sec: False
      steps:
      - run: inference 
        isolate-runs: no
        flags:
          rs_prefix: evvani1_
          # tgtdata_path: /home/serrano/projects/n2c2/tracking/.guild/runs/4a600f191bc24f7d8c442328c3f4c4a4/prepared/stage2_train.pkl
          tgtdata_path: prepared/segments.tsv
          model_path: /home/owlmx/research/comps/n2c2/models/event/CMED_clinicalsingle_new_pair
          model_type: self
          input_features: text,term
          label_mapper: 
      - run: inference 
        isolate-runs: no
        flags:
          rs_prefix: evc4dab27b_    # Bio_ClinicalBERT + text|mask_term + stratified batch
          tgtdata_path: prepared/predictions.tsv
          train: c4dab27bbdae44808261e2461d30f0e6
          model_type: emilyalsentzer/Bio_ClinicalBERT
          input_features: text
          fields_transformations: text|mask_term
      - run: inference 
        isolate-runs: no
        flags:
          rs_prefix: ev674e7f47_    # mimiciii_bert_10e_128b + text|mask_term + stratified batch
          tgtdata_path: prepared/predictions.tsv
          train: 674e7f47993d451191531e5e992e553b
          model_type: /home/owlmx/research/comps/n2c2/models/external/mimiciii_bert_10e_128b
          input_features: text
          fields_transformations: text|mask_term
      - run: inference 
        isolate-runs: no
        flags:
          rs_prefix: ev47ab65ee_    # mimiciii_roberta_10e_128b + text|mask_term + stratified batch
          tgtdata_path: prepared/predictions.tsv
          train: 47ab65eecf204caa8d7d7c29581ad0c2
          model_type: /home/owlmx/research/comps/n2c2/models/external/mimiciii_roberta_10e_128b
          input_features: text
          fields_transformations: text|mask_term               

 
# ------------------------------------------------------------------  
