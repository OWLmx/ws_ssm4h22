{"logs_path": {"default": "logs/", "type": "string"}, "use_weights_of": {"default": "", "type": "string"}, "patience": {"default": 3, "type": "int"}, "resume_from_ckpt": {"description": "If specified training resume from this checkpoint", "default": "", "type": "string"}, "task_name": {"description": "Task from where config will be used", "default": "base", "type": "string"}, "max_seq_length": {"default": 128, "type": "int"}, "train_batch_size": {"default": 32, "type": "int"}, "eval_batch_size": {"default": 32, "type": "int"}, "data_dirpath": {"description": "Path of the directory where the datafiles are located", "default": "", "type": "string"}, "data_filename_prefix": {"description": "Datafile's name without the suffix related to the split identification.", "default": "", "type": "string"}, "data_filename_type": {"description": "CSV | TSV | PKL (dataframe) ", "default": "csv", "type": "string"}, "data_split_train": {"description": "Suffix that identfies the split, if None the split won't be used", "default": "train", "type": "string"}, "data_split_valid": {"description": "Suffix that identfies the split, if None the split won't be used", "default": "valid", "type": "string"}, "data_split_test": {"description": "Suffix that identfies the split, if None the split won't be used", "default": "test", "type": "string"}, "loader_workers": {"description": "How many subprocesses to use for data loading. 0 means that                 the data will be loaded in the main process.", "default": 8, "type": "int"}, "stratified_batch_sampling": {"description": "Uses stratified batch smapling with replacement for training", "default": false, "type": "boolean"}, "fields_transformations": {"description": "Dictionary where key is the field and the value the transformation to be applied", "default": "", "type": "string"}, "tokenizer_type": {"description": "Tokenizer type for loading fine-tuned models ", "default": "", "type": "string"}, "label_encoder": {"default": "", "type": "string"}, "subsample": {"description": "To only use a randomply picked fraction of training data ", "default": 1.0, "type": "float"}, "t10sec": {"description": "10 sec minitest, useful to check that things run", "default": -1, "type": "int"}, "model_name_or_path": {"description": "Pretrained Transformer to be used.", "default": "distilbert-base-uncased", "type": "string"}, "encoder_learning_rate": {"description": "Encoder specific learning rate.", "default": 1e-05, "type": "float"}, "learning_rate": {"description": "Learning rate.", "default": 2e-05, "type": "float"}, "warmup_steps": {"description": "warmup steps.", "default": 100, "type": "int"}, "num_labels": {"description": "How many classes are expected", "default": 2, "type": "int"}, "adam_epsilon": {"description": "For optimizer numerical stability", "default": 0.001, "type": "float"}, "logger": {"description": "Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses the default ``TensorBoardLogger``. ``False`` will disable logging. If multiple loggers are provided and the `save_dir` property of that logger is not set, local files (checkpoints, profiler traces, etc.) are saved in ``default_root_dir`` rather than in the ``log_dir`` of any of the individual loggers.", "default": "yes", "type": null}, "checkpoint_callback": {"description": "If ``True``, enable checkpointing. .. deprecated:: v1.5 ``checkpoint_callback`` has been deprecated in v1.5 and will be removed in v1.7. Please consider using ``enable_checkpointing`` instead.", "type": null, "default": null}, "enable_checkpointing": {"description": "If ``True``, enable checkpointing. It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.", "default": "yes", "type": null}, "default_root_dir": {"description": "Default path for logs and weights when no logger/ckpt_callback passed. Default: ``os.getcwd()``. Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'", "type": "string"}, "gradient_clip_val": {"description": "The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.", "type": "number"}, "gradient_clip_algorithm": {"description": "The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"`` to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will be set to ``\"norm\"``.", "type": "string"}, "process_position": {"description": "Orders the progress bar when running multiple models on same machine. .. deprecated:: v1.5 ``process_position`` has been deprecated in v1.5 and will be removed in v1.7. Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``process_position`` directly to the Trainer's ``callbacks`` argument instead.", "default": 0, "type": "int"}, "num_nodes": {"description": "Number of GPU nodes for distributed training.", "default": 1, "type": "int"}, "num_processes": {"description": "Number of processes for distributed training with ``accelerator=\"cpu\"``.", "default": 1, "type": "int"}, "devices": {"description": "Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`, based on the accelerator type.", "type": "string"}, "gpus": {"description": "Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node", "type": "string"}, "auto_select_gpus": {"description": "If enabled and ``gpus`` is an integer, pick available gpus automatically. This is especially useful when GPUs are configured to be in \"exclusive mode\", such that only one process at a time can access them.", "default": "no", "type": null}, "tpu_cores": {"description": "How many TPU cores to train on (1 or 8) / Single TPU to train on [1]", "type": "string"}, "ipus": {"description": "How many IPUs to train on.", "type": "int"}, "log_gpu_memory": {"description": "None, 'min_max', 'all'. Might slow performance. .. deprecated:: v1.5 Deprecated in v1.5.0 and will be removed in v1.7.0 Please use the ``DeviceStatsMonitor`` callback directly instead.", "type": "string"}, "progress_bar_refresh_rate": {"description": "How often to refresh progress bar (in steps). Value ``0`` disables progress bar. Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.). .. deprecated:: v1.5 ``progress_bar_refresh_rate`` has been deprecated in v1.5 and will be removed in v1.7. Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``refresh_rate`` directly to the Trainer's ``callbacks`` argument instead. To disable the progress bar, pass ``enable_progress_bar = False`` to the Trainer.", "type": "int"}, "enable_progress_bar": {"description": "Whether to enable to progress bar by default.", "default": "yes", "type": null}, "overfit_batches": {"description": "Overfit a fraction of training data (float) or a set number of batches (int).", "default": 0.0, "type": "number"}, "track_grad_norm": {"description": "-1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.", "default": -1, "type": "float"}, "check_val_every_n_epoch": {"description": "Check val every n train epochs.", "default": 1, "type": "int"}, "fast_dev_run": {"description": "Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train, val and test to find any bugs (ie: a sort of unit test).", "default": "no", "type": null}, "accumulate_grad_batches": {"description": "Accumulates grads every k batches or as set up in the dict.", "type": "int"}, "max_epochs": {"description": "Stop training once this number of epochs is reached. Disabled by default (None). If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``. To enable infinite training, set ``max_epochs = -1``.", "type": "int"}, "min_epochs": {"description": "Force training for at least these many epochs. Disabled by default (None). If both min_epochs and min_steps are not specified, defaults to ``min_epochs = 1``.", "type": "int"}, "max_steps": {"description": "Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1`` and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set ``max_epochs`` to ``-1``.", "default": -1, "type": "int"}, "min_steps": {"description": "Force training for at least these number of steps. Disabled by default (None).", "type": "int"}, "max_time": {"description": "Stop training after this amount of time has passed. Disabled by default (None). The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a :class:`datetime.timedelta`, or a dictionary with keys that will be passed to :class:`datetime.timedelta`.", "type": "string"}, "limit_train_batches": {"description": "How much of training dataset to check (float = fraction, int = num_batches).", "default": 1.0, "type": "number"}, "limit_val_batches": {"description": "How much of validation dataset to check (float = fraction, int = num_batches).", "default": 1.0, "type": "number"}, "limit_test_batches": {"description": "How much of test dataset to check (float = fraction, int = num_batches).", "default": 1.0, "type": "number"}, "limit_predict_batches": {"description": "How much of prediction dataset to check (float = fraction, int = num_batches).", "default": 1.0, "type": "number"}, "val_check_interval": {"description": "How often to check the validation set. Use float to check within a training epoch, use int to check every n steps (batches).", "default": 1.0, "type": "number"}, "flush_logs_every_n_steps": {"description": "How often to flush logs to disk (defaults to every 100 steps). .. deprecated:: v1.5 ``flush_logs_every_n_steps`` has been deprecated in v1.5 and will be removed in v1.7. Please configure flushing directly in the logger instead.", "type": "int"}, "log_every_n_steps": {"description": "How often to log within steps (defaults to every 50 steps).", "default": 50, "type": "int"}, "accelerator": {"description": "Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\") as well as custom accelerator instances. .. deprecated:: v1.5 Passing training strategies (e.g., 'ddp') to ``accelerator`` has been deprecated in v1.5.0 and will be removed in v1.7.0. Please use the ``strategy`` argument instead.", "type": "string"}, "strategy": {"description": "Supports different training strategies with aliases as well custom training type plugins.", "type": "string"}, "sync_batchnorm": {"description": "Synchronize batch norm layers between process groups/whole world.", "default": "no", "type": null}, "precision": {"description": "Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16). Can be used on CPU, GPU or TPUs.", "default": 32, "type": "int", "choices": [16, 32, 64]}, "enable_model_summary": {"description": "Whether to enable model summarization by default.", "default": "yes", "type": null}, "weights_summary": {"description": "Prints a summary of the weights when training begins. .. deprecated:: v1.5 ``weights_summary`` has been deprecated in v1.5 and will be removed in v1.7. To disable the summary, pass ``enable_model_summary = False`` to the Trainer. To customize the summary, pass :class:`~pytorch_lightning.callbacks.model_summary.ModelSummary` directly to the Trainer's ``callbacks`` argument.", "default": "top", "type": "string"}, "weights_save_path": {"description": "Where to save weights if specified. Will override default_root_dir for checkpoints only. Use this if for whatever reason you need the checkpoints stored in a different place than the logs written in `default_root_dir`. Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/' Defaults to `default_root_dir`.", "type": "string"}, "num_sanity_val_steps": {"description": "Sanity check runs n validation batches before starting the training routine. Set it to `-1` to run all batches in all validation dataloaders.", "default": 2, "type": "int"}, "resume_from_checkpoint": {"description": "Path/URL of the checkpoint from which training is resumed. If there is no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint, training will start from the beginning of the next epoch. .. deprecated:: v1.5 ``resume_from_checkpoint`` is deprecated in v1.5 and will be removed in v1.7. Please pass the path to ``Trainer.fit(..., ckpt_path=...)`` instead.", "type": "string"}, "profiler": {"description": "To profile individual steps during training and assist in identifying bottlenecks.", "type": "string"}, "benchmark": {"description": "If true enables cudnn.benchmark.", "default": "no", "type": null}, "deterministic": {"description": "If ``True``, sets whether PyTorch operations must use deterministic algorithms. Default: ``False``.", "default": "no", "type": null}, "reload_dataloaders_every_n_epochs": {"description": "Set to a non-negative integer to reload dataloaders every n epochs.", "default": 0, "type": "int"}, "reload_dataloaders_every_epoch": {"description": "Set to True to reload dataloaders every epoch. .. deprecated:: v1.4 ``reload_dataloaders_every_epoch`` has been deprecated in v1.4 and will be removed in v1.6. Please use ``reload_dataloaders_every_n_epochs``.", "default": "no", "type": null}, "auto_lr_find": {"description": "If set to True, will make trainer.tune() run a learning rate finder, trying to optimize initial learning for faster convergence. trainer.tune() method will set the suggested learning rate in self.lr or self.learning_rate in the LightningModule. To use a different key set a string instead of True with the key name.", "default": "no", "type": null}, "replace_sampler_ddp": {"description": "Explicitly enables or disables sampler replacement. If not specified this will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it, you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.", "default": "yes", "type": null}, "detect_anomaly": {"description": "Enable anomaly detection for the autograd engine.", "default": "no", "type": null}, "auto_scale_batch_size": {"description": "If set to True, will `initially` run a batch size finder trying to find the largest batch size that fits into memory. The result will be stored in self.batch_size in the LightningModule. Additionally, can be set to either `power` that estimates the batch size through a power search or `binsearch` that estimates the batch size through a binary search.", "default": "no", "type": null}, "prepare_data_per_node": {"description": "If True, each LOCAL_RANK=0 will call prepare data. Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data .. deprecated:: v1.5 Deprecated in v1.5.0 and will be removed in v1.7.0 Please set ``prepare_data_per_node`` in LightningDataModule or LightningModule directly instead.", "type": null, "default": null}, "plugins": {"description": "Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.", "type": "string"}, "amp_backend": {"description": "The mixed precision backend to use (\"native\" or \"apex\").", "default": "native", "type": "string"}, "amp_level": {"description": "The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\" if ``amp_backend`` is set to \"apex\".", "type": "string"}, "move_metrics_to_cpu": {"description": "Whether to force internal logged metrics to be moved to cpu. This can save some gpu memory, but can make training slower. Use with attention.", "default": "no", "type": null}, "multiple_trainloader_mode": {"description": "How to loop over the datasets when there are multiple train loaders. In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed, and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets reload when reaching the minimum length of datasets.", "default": "max_size_cycle", "type": "string"}, "stochastic_weight_avg": {"description": "Whether to use `Stochastic Weight Averaging (SWA) <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>`_. .. deprecated:: v1.5 ``stochastic_weight_avg`` has been deprecated in v1.5 and will be removed in v1.7. Please pass :class:`~pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging` directly to the Trainer's ``callbacks`` argument instead.", "default": "no", "type": null}, "terminate_on_nan": {"description": "If set to True, will terminate training (by raising a `ValueError`) at the end of each training batch, if any of the parameters or the loss are NaN or +/-inf. .. deprecated:: v1.5 Trainer argument ``terminate_on_nan`` was deprecated in v1.5 and will be removed in 1.7. Please use ``detect_anomaly`` instead.", "type": null, "default": null}, "$dest": "args"}